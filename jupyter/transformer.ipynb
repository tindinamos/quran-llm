{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6976722-cc13-4536-b026-0901752c03d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eeyad/projects/miniconda3/envs/lab/bin/python\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc70d5b7-17e2-4e09-9ffa-cb98d6d81c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 16 # what is the maximum context length for predictions?\n",
    "max_iters =  5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 30 # 200\n",
    "n_embd = 32\n",
    "n_layer = 2\n",
    "n_head = 2\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9339632c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('quran-simple.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d8ee7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735382"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06586528",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0123456789|ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىيًٌٍَُِّْٰۖۗۘۙۚۛۜ۩\n",
      "67\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "og_vocab = sorted(list(set(text)))\n",
    "print(''.join(og_vocab))\n",
    "print(len(og_vocab))\n",
    "std_vocab = '0123456789|ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىي ۩'\n",
    "std_vocab_size = len(std_vocab)\n",
    "print(std_vocab_size)\n",
    "\n",
    "reveries_vocab = 'ًٌٍَُِّْ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93f4bfe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1|1|بِسْمِ اللَّهِ الرَّحْمَـٰنِ الرَّحِيمِ\\n1|2|الْحَمْدُ لِلَّهِ رَبِّ الْعَالَمِينَ\\n1|3|الرَّحْمَـ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8163bf72-b68c-43da-870e-44caba82492a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def NextToken(index, text):\n",
    "    if text[index] in numerics:\n",
    "        token = [text[index],]\n",
    "        j = index + 1\n",
    "        while j < len(text) and text[index] in numerics:\n",
    "            token.append(text[j])\n",
    "            j += 1\n",
    "            if text[j-1] == '|':\n",
    "                break\n",
    "        return ''.join(token), j\n",
    "    elif text[index] in std_vocab:\n",
    "        token = [text[index],]\n",
    "        j = index + 1\n",
    "        while j < len(text) and text[j] in reveries_vocab:\n",
    "            token.append(text[j])\n",
    "            j += 1\n",
    "        return ''.join(token), j\n",
    "    else:\n",
    "        return text[index], index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c09bb25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436583\n"
     ]
    }
   ],
   "source": [
    "total = []\n",
    "numerics = '0123456789|'\n",
    "i = 0\n",
    "test_text = text # text[:1000000]\n",
    "while i < len(test_text):\n",
    "    token, i = NextToken(i, test_text)\n",
    "    # print(token)\n",
    "    total.append(token)\n",
    "print(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbad73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_vocab = sorted(list(set(total)))\n",
    "actual_vocab_size = len(actual_vocab)\n",
    "actual_vocab_size, actual_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef21b7dc-e0bc-4483-808b-0b669097c888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('إِيَادْ إِبْرَاهِيمْ', 'إِيَادْ إِبْرَاهِيمْ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(actual_vocab)}\n",
    "itos = {i:ch for i,ch in enumerate(actual_vocab)}\n",
    "# encode = lambda s: [stoi[ss] for ss in s]\n",
    "def encode(text):\n",
    "    i = 0\n",
    "    total = []\n",
    "    while i < len(text): \n",
    "        token, i = NextToken(i, text)\n",
    "        total.append(stoi[token])\n",
    "    return total\n",
    "decode = lambda i: ''.join([itos[ii] for ii in i])\n",
    "\"إِيَادْ إِبْرَاهِيمْ\", decode(encode(\"إِيَادْ إِبْرَاهِيمْ\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "05e62c8b-08f5-4811-b084-68260931ba54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ord() expected a character, but string of length 4 found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text\u001b[38;5;241m.\u001b[39mtranslate({\u001b[38;5;28mord\u001b[39m(c): (c \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m actual_vocab \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m actual_vocab})[:\u001b[38;5;241m10\u001b[39m]\n",
      "Cell \u001b[0;32mIn[142], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text\u001b[38;5;241m.\u001b[39mtranslate({\u001b[38;5;28mord\u001b[39m(c): (c \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m actual_vocab \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m actual_vocab})[:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: ord() expected a character, but string of length 4 found"
     ]
    }
   ],
   "source": [
    "# text.translate({ord(c): (c if c in vocab else None) for c in vocab})[:10]\n",
    "# Not working for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e0b1c8-7cc9-44ef-a04a-0b5003c52b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([112, 112, 321, 450, 586,   1, 314, 566, 576, 613,   1, 314, 566, 424,\n",
      "        379, 584, 524, 645, 600,   1, 314, 566, 424, 376, 632, 586,   0, 112,\n",
      "        210, 314, 579, 374, 593, 394,   1, 572, 576, 613,   1, 418, 327,   1,\n",
      "        314, 579, 509, 314, 570, 586, 632, 598,   0, 112, 221, 314, 566, 424,\n",
      "        379, 584, 524, 645, 600,   1, 314, 566, 424, 376, 632, 586,   0, 112,\n",
      "        232, 584, 314, 572, 558,   1, 635, 630, 586,   1, 314, 566, 401, 632,\n",
      "        600,   0, 112, 243, 307, 641, 314, 556,   1, 598, 516, 320, 394,   1,\n",
      "        621, 307, 641, 314, 556,   1, 598, 450, 339, 511, 632, 599,   0, 112,\n",
      "        254, 314, 617, 395, 598, 314,   1, 314, 566, 470, 418, 314, 486,   1,\n",
      "        314, 579, 585, 450, 339, 544, 632, 584,   0, 112, 265, 467, 418, 314,\n",
      "        486,   1, 314, 576, 409, 632, 598,   1, 297, 606, 509, 593, 339,   1,\n",
      "        509, 570, 644, 613, 593,   1, 520, 644, 420,   1, 314, 579, 584, 523,\n",
      "        476, 618, 321,   1, 509, 570, 644, 613, 593,   1, 621, 570, 314,   1,\n",
      "        314, 566, 478, 314, 578, 632, 598,   0, 210, 112, 321, 450, 586,   1,\n",
      "        314, 566, 576, 613,   1, 314, 566, 424, 379, 584, 524, 645, 600,   1,\n",
      "        314, 566, 424, 376, 632, 586,   1, 314, 566, 580,   0, 210, 210, 407,\n",
      "        645, 572, 556,   1, 314, 579, 558, 339, 314, 320,   1, 570, 314,   1,\n",
      "        418, 644, 319,   1, 651,   1, 531, 632, 613,   1, 651,   1, 612, 390,\n",
      "        631,   1, 578, 579, 585, 342, 544, 632, 598,   0, 210, 221, 314, 576,\n",
      "        409, 632, 598,   1, 636, 305, 586, 599, 618, 598,   1, 321, 314, 579,\n",
      "        520, 644, 321,   1, 621, 636, 544, 632, 585, 618, 598,   1, 314, 566,\n",
      "        468, 570, 314, 332,   1, 621, 586, 590, 314,   1, 418, 431, 551, 598,\n",
      "        314, 612, 593,   1, 636, 594, 531, 543, 618, 598,   0, 210, 232, 621,\n",
      "        314, 576, 409, 632, 598,   1, 636, 305, 586, 599, 618, 598,   1, 321,\n",
      "        584, 314,   1, 298, 594, 433, 570,   1, 307, 570, 644, 556,   1, 621,\n",
      "        584, 314,   1, 298, 594, 433, 570,   1, 586, 594,   1, 542, 328, 572,\n",
      "        556,   1, 621, 321, 314, 579, 294, 385, 418, 334,   1, 612, 593,   1,\n",
      "        636, 618, 544, 599, 618, 598,   0, 210, 243, 298, 618, 570, 524, 645,\n",
      "        312, 556,   1, 509, 570, 631, 645,   1, 612, 390, 631,   1, 592, 594,\n",
      "          1, 424, 327, 613, 593,   1, 646,   1, 621, 298, 618, 570, 524, 645,\n",
      "        312, 556,   1, 612, 585,   1, 314, 579, 585, 537, 572, 375, 618, 598,\n",
      "          0, 210, 254, 307, 603,   1, 314, 576, 409, 632, 598,   1, 556, 529,\n",
      "        419, 618, 314,   1, 443, 621, 314, 289,   1, 509, 570, 644, 613, 593,\n",
      "          1, 297, 297, 594, 407, 427, 339, 612, 593,   1, 297, 593,   1, 570,\n",
      "        593,   1, 340, 594, 409, 427, 612, 593,   1, 570, 314,   1, 636, 305,\n",
      "        586, 599, 618, 598,   0, 210, 265, 383, 339, 584,   1, 314, 566, 576,\n",
      "        612,   1, 509, 570, 631, 645,   1, 543, 571, 618, 321, 613, 593,   1,\n",
      "        621, 509, 570, 631, 645,   1, 443, 593, 511, 613, 593,   1, 646,   1,\n",
      "        621, 509, 570, 631, 645,   1, 297, 328, 465, 314, 420, 613, 593,   1,\n",
      "        522, 454, 314, 621, 330,   1, 646,   1, 621, 570, 612, 593,   1, 509,\n",
      "        407, 314, 317,   1, 509, 498, 632, 582,   0, 210, 276, 621, 586, 598,\n",
      "          1, 314, 566, 603, 314, 445,   1, 584, 594,   1, 635, 543, 618, 571,\n",
      "          1, 294, 584, 603, 314,   1, 321, 314, 566, 576, 613,   1, 621, 321,\n",
      "        314, 579, 635, 630, 586,   1, 314, 579, 294, 385, 420,   1, 621, 584,\n",
      "        314,   1, 612, 580,   1, 321, 585, 305, 586, 600, 632, 598,   0, 210,\n",
      "        287, 636, 383, 314, 395, 510, 618, 598,   1, 314, 566, 576, 611,   1,\n",
      "        621, 314, 576, 409, 632, 598,   1, 294, 584, 599, 618, 314,   1, 621,\n",
      "        584, 314,   1, 635, 388, 393, 510, 618, 598,   1, 307, 576, 314,   1,\n",
      "        297, 594, 530, 443, 612, 593,   1, 621, 584, 314,   1, 635, 460, 510,\n",
      "        419, 618, 598,   0, 210,  12, 531, 632,   1, 543, 571, 618, 321, 613,\n",
      "        580,   1, 590, 418, 473,   1, 529, 431, 314, 393, 612, 585,   1, 314,\n",
      "        566, 576, 612,   1, 584, 418, 472, 314,   1, 646,   1, 621, 570, 612,\n",
      "        593,   1, 509, 407, 314, 317,   1, 297, 572, 632, 582,   1, 321, 584,\n",
      "        314,   1, 556, 314, 599, 618, 314,   1, 635, 565, 409, 320, 618, 598,\n",
      "          0, 210,  23, 621, 307, 407, 314,   1, 544, 632, 570,   1, 570, 612,\n",
      "        593,   1, 570, 314,   1, 340, 537, 445, 394, 618, 314,   1, 531, 632,\n",
      "          1, 314, 579, 297, 427, 477,   1, 542, 314, 571, 618, 314,   1, 307,\n",
      "        603, 584, 314,   1, 598, 379, 599,   1, 585, 471, 572, 375, 618, 598,\n",
      "          0, 210,  34, 297, 570, 314,   1, 307, 603, 612, 593,   1, 612, 585,\n",
      "          1, 314, 579, 585, 537, 445, 394, 618, 598,   1, 621, 570, 524, 645,\n",
      "        558, 594,   1, 576, 314,   1, 635, 460, 510, 419, 618, 598,   0, 210,\n",
      "         45, 621, 307, 407, 314,   1, 544, 632, 570,   1, 570, 612, 593,   1,\n",
      "        294, 586, 599, 618, 314,   1, 556, 584, 314,   1, 294, 584, 598,   1,\n",
      "        314, 566, 603, 314, 444,   1, 542, 314, 571, 618, 314,   1, 297, 599,\n",
      "        305, 586, 599,   1, 556, 584, 314,   1, 294, 584, 598,   1, 314, 566,\n",
      "        448, 529, 611, 314, 292,   1, 647,   1, 297, 570, 314,   1, 307, 603,\n",
      "        612, 593,   1, 612, 585,   1, 314, 566, 448, 529, 611, 314, 292,   1,\n",
      "        621, 570, 524, 645, 558, 594,   1, 576, 314,   1, 635, 516, 570, 585,\n",
      "        618, 598,   0, 210,  56, 621, 307, 407, 314,   1, 570, 543, 618, 314,\n",
      "          1, 314, 576, 409, 632, 598,   1, 294, 584, 599, 618, 314,   1, 542,\n",
      "        314, 571, 618, 314,   1, 294, 584, 603, 314,   1, 621, 307, 407, 314,\n",
      "          1, 383, 570, 630, 314,   1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b323c0b5-0168-483a-87e9-38c8383d5f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "# revisit this. Quran is complicated and end is probably the shorter verses which means training and validation are already diverting.\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ef3c0d-51ed-4f7b-b003-eeb8e5ae185f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([112, 112, 321, 450, 586,   1, 314, 566, 576])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a101f8c-9688-4440-89f0-49fd859a390a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([112])  and target:  tensor(112)\n",
      "input:  tensor([112, 112])  and target:  tensor(321)\n",
      "input:  tensor([112, 112, 321])  and target:  tensor(450)\n",
      "input:  tensor([112, 112, 321, 450])  and target:  tensor(586)\n",
      "input:  tensor([112, 112, 321, 450, 586])  and target:  tensor(1)\n",
      "input:  tensor([112, 112, 321, 450, 586,   1])  and target:  tensor(314)\n",
      "input:  tensor([112, 112, 321, 450, 586,   1, 314])  and target:  tensor(566)\n",
      "input:  tensor([112, 112, 321, 450, 586,   1, 314, 566])  and target:  tensor(576)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "x, y\n",
    "context = []\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    print('input: ', context, ' and target: ', y[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "062bbebb-c53c-45db-853c-76ae37eeb014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[  1, 646,   1, 621, 314, 342, 543, 618],\n",
      "        [618, 598,   1, 529, 341, 632, 567, 314],\n",
      "        [585, 618, 443, 631, 645,   0, 123, 241],\n",
      "        [  1, 529, 570, 314,   1, 339, 413, 611]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[646,   1, 621, 314, 342, 543, 618, 314],\n",
      "        [598,   1, 529, 341, 632, 567, 314,   0],\n",
      "        [618, 443, 631, 645,   0, 123, 241, 529],\n",
      "        [529, 570, 314,   1, 339, 413, 611, 328]])\n",
      "----\n",
      "when input is [1] the target: 646\n",
      "when input is [1, 646] the target: 1\n",
      "when input is [1, 646, 1] the target: 621\n",
      "when input is [1, 646, 1, 621] the target: 314\n",
      "when input is [1, 646, 1, 621, 314] the target: 342\n",
      "when input is [1, 646, 1, 621, 314, 342] the target: 543\n",
      "when input is [1, 646, 1, 621, 314, 342, 543] the target: 618\n",
      "when input is [1, 646, 1, 621, 314, 342, 543, 618] the target: 314\n",
      "when input is [618] the target: 598\n",
      "when input is [618, 598] the target: 1\n",
      "when input is [618, 598, 1] the target: 529\n",
      "when input is [618, 598, 1, 529] the target: 341\n",
      "when input is [618, 598, 1, 529, 341] the target: 632\n",
      "when input is [618, 598, 1, 529, 341, 632] the target: 567\n",
      "when input is [618, 598, 1, 529, 341, 632, 567] the target: 314\n",
      "when input is [618, 598, 1, 529, 341, 632, 567, 314] the target: 0\n",
      "when input is [585] the target: 618\n",
      "when input is [585, 618] the target: 443\n",
      "when input is [585, 618, 443] the target: 631\n",
      "when input is [585, 618, 443, 631] the target: 645\n",
      "when input is [585, 618, 443, 631, 645] the target: 0\n",
      "when input is [585, 618, 443, 631, 645, 0] the target: 123\n",
      "when input is [585, 618, 443, 631, 645, 0, 123] the target: 241\n",
      "when input is [585, 618, 443, 631, 645, 0, 123, 241] the target: 529\n",
      "when input is [1] the target: 529\n",
      "when input is [1, 529] the target: 570\n",
      "when input is [1, 529, 570] the target: 314\n",
      "when input is [1, 529, 570, 314] the target: 1\n",
      "when input is [1, 529, 570, 314, 1] the target: 339\n",
      "when input is [1, 529, 570, 314, 1, 339] the target: 413\n",
      "when input is [1, 529, 570, 314, 1, 339, 413] the target: 611\n",
      "when input is [1, 529, 570, 314, 1, 339, 413, 611] the target: 328\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03fa35ab-88fc-4d81-887e-5b933a6c2ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(4337)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(n_head, n_embd // n_head)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(actual_vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.f_ln = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, actual_vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.f_ln(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daec39b2-e8b6-4756-ba71-201779578d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "667928d5-116a-4144-bd40-2edbaeb61730",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.6282, val loss 6.6065\n",
      "step 500: train loss 4.2329, val loss 4.2826\n",
      "step 1000: train loss 3.9348, val loss 4.0533\n",
      "step 1500: train loss 3.7482, val loss 3.8845\n",
      "step 2000: train loss 3.5680, val loss 3.7648\n",
      "step 2500: train loss 3.3012, val loss 3.5405\n",
      "step 3000: train loss 3.3508, val loss 3.5407\n",
      "step 3500: train loss 3.2883, val loss 3.6330\n",
      "step 4000: train loss 3.1544, val loss 3.4824\n",
      "step 4500: train loss 3.2039, val loss 3.4884\n",
      "running on cuda\n",
      "\n",
      "18|اذٌمُابٍزٌ اللَّذِزِ108|وَأَطْو220|هُمْ مُّأْٰيَضَّي مَّاصِّي أَنْا فِي ظَ لَهُكَ عَيَثٍ أُغْبْرُمَا كَنَ فَّتَسْكَا تَمَا وْمِ الْيْنْ يَمْ مِنَّ ۖ فَهَا إِنَّ عَلَ يَدِدْنَعِ بِمَا بِلَمَا يَالْمُدَّا مُّالْأَمْ تَمُتَ هُوَيُقِا أَرْهَّهُ فَاوَادُ مِنْا أَبِهَا حَذْنَّالِدِمْ ۖ وَتَرَبُّ إِفَاكَا إِنْا مَهُمْ كُمْ بِدِحِمُ 66|لْعَأُودَّا حْآيَوْهِ لَ رُورَوْ أَ56|يَقَافَؤُسَأَن اللَّادِصَوْهَ نُبَجْلَا خَونَ فِيوَالرْظُنَهُم مُّينَكُذٍينَ مَحَوْيَٰ ۖ إِةِ أَعُ وَوَالَّهُمْ لَّا ۖ وَبِوَحًعُ وَآفِرَّرًا وَتَقُىٰ كَمَانِينَ فِيرَيْهِ آلَّينَ وَلَ يَجْرُ ۚ وَأَكَمَ عَنِّبِّا ۚ لَا بِنُ رِّرَٰطَّيرُمْ فَوَأَبُلَّاتٍزِهُمْ لِّتَتَّرْحُنَ\n",
      "242|اضِ عَمُينَ\n",
      "26|حُوَمَنُّبِوْقُ وَانٌنُولَيْعًمْهُمْ لَمُّا آرَؤْمِ إِلَا مِّعَ47|يَنَّ ۗينْنُوعْهُ ذُلَرُلِمُ ۖ الْدَإِنَّ أَرٍكَ الَّذِي أَعْؤُانَ التَأَرْمِعْسَ يُونَ وَقَصِ ۖ لَّذِيدِنَ\n",
      "20|سْكُّأَأَندِ بِنْا\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "print('running on %s' % device)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9b8a3ddc-38b5-482f-8edf-14685a082485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "42|51|4|88|اللَّهُ فِي هَنِ افِيمٌ كَذَالَّطَا جَرَجَعَظِي ۖ عَذَالْكَالْمُوالنَّبْلِنَ الْبَىٰ تَيْنِ سُوتَحَكَا خَلْيَةٍ كَا مَلُونَ كَا عَلَاللَّ يُجْرِ أَوْفَرَّبِّكَهْدِ يَجِئْتَاللَّابُلًانِينَ\n",
      "47|وَإِلَّهُمْرِي وَإِنَّجَّ187|فَكُرْقُصُّمَّ الْآوَقَالْتَ ا عَرْضِي وَأَعْدِ فَمَـٰ أَنَّاطِيلٍ مَّ فِلَاتَ مَ ۚ تُغْ ۖ مِنْ وَأَن ا جُرُونَ ا وَمَنُسَلْأَأَلَالَّذِيدِي إِبْخَذَٰ عِن عَلِلَّهُمْعَلَـٰلِلَّا أَعْبُوطًالْحَمِنكُمْ نَ عَ مَغْرَحْمَالْحَقَّقُرْهَ ۖ تُمْ نَ وَهُمْ ۚ تَا أَرْطَ جَوَالرۛ وَكَاللَّهُ فِيثَاللَّذِرَاءَتْرَبِّكُ بِهِ وَمَكْرٍ وَذَكَّنَّالْحِي قُوَّلِلسَّمَرَجَزَلَيْنَ\n",
      "12|36|21|23|37|79|كَمَسْتَعَهُمُ\n",
      "31|قُلُولُوا ادْخِيهَ وَ الرِّمَادَ أَلْمُونَصِرْضِ أَكُلُوسَطُرَّبُّكَمَا ۚ وَّلِ يَظْلِكَةَ لِقَوْلًا خَذْنَالنَّهُمْرِيلًالصَّا ۚ ذُ ضِيَهُ قَدْعُ عَلْيُن مَّعَلَّ إِنَّمُمَـٰهِ مِنتَذَٰ اللَّذِيمٌ\n",
      "5|63|ا أَخَلَوْلَـٰ ا عَالْ عَلَكْرًا ۚ وَمَلُوابِيلًالَّهِ ا كَ ۚ حَا أَفَسَالنَّ تَفْتِ وَسَفَأْوَجَلِكَ قُ تَخَلَىٰكِرٍ رُكُمْ فَرِ\n",
      "40|إِلَّ فَ بِبَارَ كَ امَ وَ قَدْعُوفِينَاللَّهِ عَلُ ا نَّ رُوحَسَبِا ا أَفَلِا فَاللَّىٰ هُوَهِ وَإِذًىٰكِيرًا دُوا بَادِ فَأَيَّنَالدُّ نَّكُمْ أَعِقَا إِنَّمَابَيْنِ حَنِ أَصْبَلَهُ لَهُوَلَيْمَا إِذَانَ قَوْقَوْنَ جَزَلْمَ جَعَسَعَلَ ۚ مِنَبِيهَاعِبَعُ اتَّى رِجٍ ۚ حَاسِ مِي مُمْتُهَالسُّونَا الِكَبِآيَتَعْرِرْضِهِ وَالَ تَمَا وَ شَا شَةَ عَفُودًا فَقَوْالصَّدَرَبِّئْنَىٰلِينَا إِنَّ ۖ ۙ ۖ وَشُرَالظَّالنَّ مِنْهَا آمَ فِي كُمْ ائِكَ أَلَكُلُنَا غَيْئًا أَنفُونَ إِلَّذِينَ فِضَّلَمْ عِن عِبَانُونَ الَّذِي كُلِّكَذَا آ بِعُ آلِينَ اتِي مِنْ ا يَزِيضُوانُسَاللَّ يُصِرَبَّكَ يُؤْمِرْآخِلُ اسَلْقَوْا وَعَلِ بِرِيحٌ امَارَاءُ اتُبٍ بَرُوجُ نَ إِنَّ هُ فَلَوْمِنَا الطَّيْنَ أَرْعَا لَائِنَّا ۗ\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6c359270-f5b2-4694-a9bc-187c3dfc0394",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2|203|قَ مَلْحَمُعِلَّقَدْتِ وَإِلُوا يُجْتَشَيْنَ\n",
      "16|رُّدِنَ\n",
      "26|كْسَ40|الَـٰفِ خَلِيمٌ حَيَا آمَنَا اللَّهَهُ فِ نِمَّلِرَ سَلْمَدُّنُ يَغُدَّقُ طَفَزَقَاسَلُونَ آلُ نَظْ عَ90|هِمْفِ يُضٍملُونَ\n",
      "27|وَكَهَا بَمَاهْ البَرُ كَامٌالْآخُخْبِخُنِ بِهِ فِهَحْهُ ۚ أَيَوْمِ يَرُودٍوا سَلَّكُمْ تَسْلٍ نَحْيَمْ مِّنًاهُ أَبْلَا مَا مُّتَيْتَنَّ بِكَالَهُ مِن الظَّ\n",
      "2|63|وَأَنسِوَتِنَا الَّذِينَ\n",
      "75|41|6|وَمَا ۚرُ\n",
      "26|يُطَكَثِرًا مِّلُوا لِيُرًا كَا وَمَن عَنَا مَن يُسَقْيكَ\n",
      "9|وَجُهِمْدُ يَـقَابٌ خُنْ يَعْتُمْ إِذَارِ فِرِّ آيَثْلِرِّيًّىٰ حَسْتَرَهُ لَىٰ فِيرْسَنِّينَ أَنتُضُونَ\n",
      "دِ15|26|نَّهُم وَمُوعِينَ السَّهِي تَشَشْكَ أَلْأَرْرِيَا نَالَّهُم الْ أَقْرَىٰذَهُرْمْ آتَأْتِدَّهُ الْبَسْرَ 106|يَجْإِنَّكُ هُمْ الَّذِيمُ هُمْ وَعَفْزَالُونَ\n",
      "2|18|لَّهَ تَ مُوا اتَّنُوا اللَّذِيرُ أَرْكَانَ\n",
      "39|21|طَّمَا بَنَّدْمِيرٍ\n",
      "15|47|وْخَ بِصِيفُ مِن يُشْينَ\n",
      "34|ابٍ\n",
      "43|سَتَفْ مَلُونَهَا وَقُرَ وَأَقْاوَدُونَ\n",
      "9|9|47|وَقَلُونَ كَ ابَرْلْصُولْكُ124|دَبَ أَرْنِينِ وَعْ أُوا خَتَزْلَكَثِينَ\n",
      "كَفُبَا ۗ وَأَناكِنَ\n",
      "19|13|نٍ12|وَهُقُوفِّ يُمَنِيلِ مُؤْونَ\n",
      "31|190|اءَ يَنتُمُوشِ مُّدِيتُ عَذِيدِ الْفُوا دَرَجُإٍ مَا ۚ وَقَسِ اللْعَذُذُّ تَهْ لَيَا مَن ذَالِ لَا أَيُّ بِآلِمَا وَلَيْضَ اضِمَا الْيَعْدِيهِمْ فَلْجَابِزَتْ بَعْأَشْعُ لِّعْلَيْينَا أَمَنْ حَاللَّذِ ۖ مِءَ أَجَن إِلَّهُمْ يَتَّتُبَعْ لَيْسَصُوالَ غَابُ مُّطَا فِينَ مَانَ رُمَ لَا الصَّا مِنْ عَتَّنَسَوْ أَسْثَرْفَأَن عِنظُمُرُ مِّلْمُجْتَعَحْوبَقِينِ لَلُ مَبَّلُّ الَّذِهَقَهُمَّامًا رَحِي فَأَلَيْ أَسْتَخْأُ مِنِينَ يُؤْيَ عَبِرَرَهُمْ أَكْحَ قَاللَّهُمْ ۖ وَلَا أُلْ أَدَّضْ هَامِن ۖ لَى ثُلُهُمْ عَلَا يَا وَأَبَادَامْ اللَّا أُولُ مَّا إِيْنَا مَظٍرْلٍّصِطَجِ إِنْا مَلُ أَدْدَ عَبَاكَ أَنرَانِ أَقْرِيِّ قَـٰلِكَ الْ وَالْقَّنَّ فِينَ\n",
      "2|يْفَرَبُّ الَاسِرِينَ\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
